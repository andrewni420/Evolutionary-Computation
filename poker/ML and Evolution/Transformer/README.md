# Transformer Paper Descriptions
## Attention is All You Need
Pioneer paper in transformers. 
Explains why transformers are better than LSTMs
Uses a 6xEncoder -> 6xDecoder architecture
## BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding
## Decision Transformer - Reinforcement Learning via Sequence Modeling
First paper to apply transformers to RL. 
Offline RL formulated as a sequence modeling problem - model the sequence (R S A ...) obtained from random walk where R = reward-to-go, S = state, A = action
Trained to accurately predict A given previous sequence.
Able to display different skill levels during inference by controlling the value of R
## Deep Reinforcement Learning with Swin Transformer
## Deep Transformer Q-Networks for Partially Observable Reinforcement Learning
## Dropout - A Simple Way to Prevent Neural Networks from Overfitting
## Improving Language Understanding by Generative Pre-Training
## Language Models are Few Shot Learners
## Language Models are Unsupervised Multitask Learners
## Multi-Agent Reinforcement Learning is A Sequence Modeling Problem
## On Transforming Reinforcement Learning by Transformer - The Development Trajectory
## Rapid Task-Solving in Novel Environments
## Recursive Transformer- A Novel Neural Architecture for Generalizable Mathematical Reasoning
## Sequence to Sequence Learning with Neural Networks
## Stabilizing Transformers for Reinforcement Learning
## The Sensory Neuron as a Transformer- Permutation-Invariant Neural Networks for Reinforcement Learning
## Transformer Based Reinforcement Learning For Games
## Transformer Models - An Introduction and Catalog
## Transformer-based Working Memory for Multiagent Reinforcement Learning with Action Parsing
