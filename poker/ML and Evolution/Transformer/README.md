# Transformer Paper Descriptions
## [Attention is All You Need](Attention%20is%20All%20You%20Need.pdf)
Pioneer paper in transformers. 

Explains why transformers are better than LSTMs

Uses a 6xEncoder -> 6xDecoder architecture
## BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding
## Decision Transformer - Reinforcement Learning via Sequence Modeling
First paper to apply transformers to RL. 

Offline RL formulated as a sequence modeling problem - model the sequence (R S A ...) obtained from random walk where R = reward-to-go, S = state, A = action

Trained to accurately predict A given previous sequence.

Able to display different skill levels during inference by controlling the value of R
## Deep Reinforcement Learning with Swin Transformer
## Deep Transformer Q-Networks for Partially Observable Reinforcement Learning
## Dropout - A Simple Way to Prevent Neural Networks from Overfitting
A description of dropout, where weights in a network are randomly set to zero during each training iteration. Helps prevent weights from coadapting, helps prevent the network from being too brittle, helps the network learn generalizable features.
Dropout is used in transformers
## Improving Language Understanding by Generative Pre-Training
GPT2 architecture - 12x transformer decoder layer
## Language Models are Few Shot Learners
## Language Models are Unsupervised Multitask Learners
## Multi-Agent Reinforcement Learning is A Sequence Modeling Problem
## On Transforming Reinforcement Learning by Transformer - The Development Trajectory
## Rapid Task-Solving in Novel Environments
## Recursive Transformer- A Novel Neural Architecture for Generalizable Mathematical Reasoning
## Sequence to Sequence Learning with Neural Networks
## Stabilizing Transformers for Reinforcement Learning
## The Sensory Neuron as a Transformer- Permutation-Invariant Neural Networks for Reinforcement Learning
## Transformer Based Reinforcement Learning For Games
## Transformer Models - An Introduction and Catalog
Catalogs and introduces different types of transformers. 
Explains different pretraining methods
## Transformer-based Working Memory for Multiagent Reinforcement Learning with Action Parsing
